{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07b965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install regex\n",
    "\n",
    "### REFERENCES\n",
    "#https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "#https://github.com/christianversloot/machine-learning-articles/blob/main/creating-a-multilayer-perceptron-with-pytorch-and-lightning.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f12cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Dependencies\n",
    "\"\"\"\n",
    "contractions        NA\n",
    "gensim              4.3.0\n",
    "nltk                3.8.1\n",
    "numpy               1.23.5\n",
    "pandas              1.5.3\n",
    "session_info        1.0.0\n",
    "sklearn             1.2.1\n",
    "torch               1.12.1\n",
    "torchvision         0.13.1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9c31f",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6a79ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vatsa\\anaconda3\\envs\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing all the necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import nltk\n",
    "import os\n",
    "import html\n",
    "import re\n",
    "import contractions\n",
    "import torch\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from itertools import chain\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff40231",
   "metadata": {},
   "source": [
    "## 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88aa168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "df = pd.read_table('data.tsv', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc46cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the no of null values\n",
    "df.isna().any(axis=1).sum()\n",
    "\n",
    "# Dropping NULL rows\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39129f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Converting Star Rating into three categories\\ndf['star_rating'] = df['star_rating'].replace([2,3,4,5,'1','2','3','4','5'],[1,2,3,3,1,1,2,3,3])\\n\\n# Removing some irrelevant rows in star rating feature\\ndf = df[df['star_rating'].isin([1,2,3])]\\n\\n#Converting the datatype from Object to INT\\ndf['star_rating'] = df['star_rating'].astype(str).astype(float).astype(int)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting Star Rating into three categories\n",
    "df['star_rating'] = df['star_rating'].replace([2,3,4,5,'1','2','3','4','5'],[1,2,3,3,1,1,2,3,3])\n",
    "\n",
    "# Removing some irrelevant rows in star rating feature\n",
    "df = df[df['star_rating'].isin([1,2,3])]\n",
    "\n",
    "#Converting the datatype from Object to INT\n",
    "df['star_rating'] = df['star_rating'].astype(str).astype(float).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b43c3263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Sampling\n",
    "df = df.groupby('star_rating', group_keys=False).apply(lambda x: x.sample(20000))\n",
    "df.index = (np.arange(60000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff0b2c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the Sampled Dataset\n",
    "df.to_csv('Sampled_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5bc022",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Sampled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f4005ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # convert html escapes like &amp; to characters.\n",
    "    text = html.unescape(text)\n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # <3 to love\n",
    "    text = re.sub(r'<3', 'love', text)\n",
    "    # Removing \\\\\"\n",
    "    text = re.sub(r'\\\\\"', '', text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7876ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm(word_tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in word_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61c4b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_puctuations(text):\n",
    "  # Remove punctuation\n",
    "  text = re.sub(r'[^\\w\\s]', '', text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d120f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the dataset \n",
    "df = df[['star_rating', 'review_body']]\n",
    "df['review_body'] = df['review_body'].map(lambda x: contractions.fix(x))\n",
    "df['review_body'] = df['review_body'].map(clean)\n",
    "df['review_body'] = df['review_body'].map(remove_puctuations)\n",
    "df['review_body'] = df['review_body'].map(lambda x: nltk.word_tokenize(x)) \n",
    "df['review_body'] = df['review_body'].map(lm) \n",
    "\n",
    "# Splitting the dataset\n",
    "X = df.drop(columns='star_rating')\n",
    "y = df['star_rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b83e731",
   "metadata": {},
   "source": [
    "## 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b21244",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3977b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Google Word2Vec Model\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baed2f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.5815584063529968)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['man', 'lady'], negative=['boy'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75336a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shampoos', 0.7020988464355469),\n",
       " ('moisturizer', 0.6901729106903076),\n",
       " ('serums', 0.6792107224464417),\n",
       " ('lotion', 0.668178915977478),\n",
       " ('shampoo_conditioner', 0.6428859233856201)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['serum', 'shampoo'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "287c9f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'car'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44be91b",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "600f189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training our own Word2Vec Model\n",
    "model = gensim.models.Word2Vec(sentences=df[\"review_body\"],min_count=9,vector_size=300,window=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f779839c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6743051409721375)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['man', 'lady'], negative=['boy'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5ac0a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conditioner', 0.7931665182113647),\n",
       " ('Shampoo', 0.7175987958908081),\n",
       " ('toner', 0.7115134596824646),\n",
       " ('vitamin', 0.667493462562561),\n",
       " ('product', 0.6660668849945068)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['serum', 'shampoo'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bdd8d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sea'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b885c",
   "metadata": {},
   "source": [
    "**Q:** What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better? <br>\n",
    "**Ans:** The vectors generated from pretrained model are more accurate than the model that is trained on the dataset. The pretrained model covers a wide range of words as it is trained on a large corpus. For Example: \"King - Man + Woman = Queen\" will give a key error in my trained Word2Vec model beacuse it only has a limited amount of words for training. As the pretrained model has many words in its dictionary it is encoding semantic similarities better between words. Another Example is the doesn't match in the list ['fire', 'water', 'land', 'sea', 'air', 'car' ]. It should be car because all other are natural objects. The pretrained model is able to find the difference but my trained model doesn't. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45010f49",
   "metadata": {},
   "source": [
    "## 3. Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "447d09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_word_embeddings(df):\n",
    "    \"\"\"\n",
    "    Function to get the mean Embedding vector\n",
    "    \"\"\"\n",
    "    word_embeddings = []\n",
    "    for r in df[\"review_body\"]:\n",
    "        vec = np.zeros(300)\n",
    "        c = 0\n",
    "        for word in r:\n",
    "            try:\n",
    "                vec += wv[word] \n",
    "                c += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if c>0:\n",
    "            word_embeddings.append(vec/c)\n",
    "        else:\n",
    "            word_embeddings.append(vec)\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2c05a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_mean_word_embeddings(X_train)\n",
    "x_te = get_mean_word_embeddings(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ee96766",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Word2Vec Perceptron:  0.6315833333333334\n",
      "Accuracy of TF-IDF Perceptron: 0.6459166666666667\n"
     ]
    }
   ],
   "source": [
    "# Training and Evaluating Perceptron\n",
    "perceptron = Perceptron(random_state=25)\n",
    "perceptron.fit(x,y_train)\n",
    "y_pred = perceptron.predict(x_te)\n",
    "y = classification_report(y_pred,y_test,output_dict=True)\n",
    "print(\"Accuracy of Word2Vec Perceptron: \",y[\"accuracy\"])\n",
    "print(\"Accuracy of TF-IDF Perceptron: 0.6459166666666667\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db08f174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Word2Vec SVC:  0.6504166666666666\n",
      "Accuracy of TF-IDF SVC: 0.7014166666666667\n"
     ]
    }
   ],
   "source": [
    "# Training and Evaluating SVM\n",
    "svc = LinearSVC(random_state = 25)\n",
    "svc.fit(x,y_train)\n",
    "y_pred = svc.predict(x_te)\n",
    "y = classification_report(y_pred,y_test,output_dict=True)\n",
    "print(\"Accuracy of Word2Vec SVC: \",y[\"accuracy\"])\n",
    "print(\"Accuracy of TF-IDF SVC: 0.7014166666666667\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b07ca2",
   "metadata": {},
   "source": [
    "**Q:** What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained Word2Vec features)? <br>\n",
    "**Ans:** TF-IDF features performed better than the Word2Vec feature on both Perceptron and SVM Model on the accuracy score. TF-IDF outperformed SVM by about 1.4% on Perceptron and around 5% on SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af8b91",
   "metadata": {},
   "source": [
    "## 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de527665",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64d6973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(300, 100).cuda(),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(100, 10).cuda(),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(10, 3).cuda(),\n",
    "      #nn.Dropout(0.2)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83bfbafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21bc80d5970>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set fixed random number seed\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab1cc17",
   "metadata": {},
   "source": [
    "### a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e2d5208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuple(x,y):\n",
    "    tmp = []\n",
    "    for i in range(len(x)):\n",
    "        tmp.append((x[i],y.iloc[i]-1))\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6dc9bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(get_tuple(x,y_train),batch_size=64, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72a7a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLP\n",
    "mlp = MLP()\n",
    "mlp = mlp.cuda()\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f3cd9cba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss After Epoch 0  : 0.885559449672699\n",
      "Starting epoch 2\n",
      "Loss After Epoch 1  : 0.7890025011698405\n",
      "Starting epoch 3\n",
      "Loss After Epoch 2  : 0.7708496094544729\n",
      "Starting epoch 4\n",
      "Loss After Epoch 3  : 0.759677870353063\n",
      "Starting epoch 5\n",
      "Loss After Epoch 4  : 0.7490026630560557\n",
      "Starting epoch 6\n",
      "Loss After Epoch 5  : 0.7406753311157227\n",
      "Starting epoch 7\n",
      "Loss After Epoch 6  : 0.7340897517999013\n",
      "Starting epoch 8\n",
      "Loss After Epoch 7  : 0.727361788113912\n",
      "Starting epoch 9\n",
      "Loss After Epoch 8  : 0.7204268963734309\n",
      "Starting epoch 10\n",
      "Loss After Epoch 9  : 0.7150329933961233\n",
      "Starting epoch 11\n",
      "Loss After Epoch 10  : 0.7104324873685837\n",
      "Starting epoch 12\n",
      "Loss After Epoch 11  : 0.7052837599515915\n",
      "Starting epoch 13\n",
      "Loss After Epoch 12  : 0.7013296282291412\n",
      "Starting epoch 14\n",
      "Loss After Epoch 13  : 0.6947267072598139\n",
      "Starting epoch 15\n",
      "Loss After Epoch 14  : 0.6894266964991888\n",
      "Starting epoch 16\n",
      "Loss After Epoch 15  : 0.6851903539498647\n",
      "Starting epoch 17\n",
      "Loss After Epoch 16  : 0.681825616200765\n",
      "Starting epoch 18\n",
      "Loss After Epoch 17  : 0.6770760146776835\n",
      "Starting epoch 19\n",
      "Loss After Epoch 18  : 0.67150446164608\n",
      "Starting epoch 20\n",
      "Loss After Epoch 19  : 0.6663672264814376\n",
      "Starting epoch 21\n",
      "Loss After Epoch 20  : 0.6648269352515539\n",
      "Starting epoch 22\n",
      "Loss After Epoch 21  : 0.6579189052581788\n",
      "Starting epoch 23\n",
      "Loss After Epoch 22  : 0.6533130315144857\n",
      "Starting epoch 24\n",
      "Loss After Epoch 23  : 0.6491346496740977\n",
      "Starting epoch 25\n",
      "Loss After Epoch 24  : 0.6437930430173874\n",
      "Starting epoch 26\n",
      "Loss After Epoch 25  : 0.6403376007477443\n",
      "Starting epoch 27\n",
      "Loss After Epoch 26  : 0.637038424372673\n",
      "Starting epoch 28\n",
      "Loss After Epoch 27  : 0.6325810817480088\n",
      "Starting epoch 29\n",
      "Loss After Epoch 28  : 0.6289569786389668\n",
      "Starting epoch 30\n",
      "Loss After Epoch 29  : 0.6241295913060506\n",
      "Starting epoch 31\n",
      "Loss After Epoch 30  : 0.6183254792690277\n",
      "Starting epoch 32\n",
      "Loss After Epoch 31  : 0.6169235920508702\n",
      "Starting epoch 33\n",
      "Loss After Epoch 32  : 0.6113808862765631\n",
      "Starting epoch 34\n",
      "Loss After Epoch 33  : 0.606227180202802\n",
      "Starting epoch 35\n",
      "Loss After Epoch 34  : 0.6023411455551784\n",
      "Starting epoch 36\n",
      "Loss After Epoch 35  : 0.5984515424966812\n",
      "Starting epoch 37\n",
      "Loss After Epoch 36  : 0.5941748328208923\n",
      "Starting epoch 38\n",
      "Loss After Epoch 37  : 0.5907773649692536\n",
      "Starting epoch 39\n",
      "Loss After Epoch 38  : 0.5879596176147461\n",
      "Starting epoch 40\n",
      "Loss After Epoch 39  : 0.5839610399405162\n",
      "Starting epoch 41\n",
      "Loss After Epoch 40  : 0.5800667163133622\n",
      "Starting epoch 42\n",
      "Loss After Epoch 41  : 0.5727790543238322\n",
      "Starting epoch 43\n",
      "Loss After Epoch 42  : 0.568867885629336\n",
      "Starting epoch 44\n",
      "Loss After Epoch 43  : 0.5666461619933446\n",
      "Starting epoch 45\n",
      "Loss After Epoch 44  : 0.5621707042058309\n",
      "Starting epoch 46\n",
      "Loss After Epoch 45  : 0.5589504307508468\n",
      "Starting epoch 47\n",
      "Loss After Epoch 46  : 0.5561024176279704\n",
      "Starting epoch 48\n",
      "Loss After Epoch 47  : 0.5528057281970977\n",
      "Starting epoch 49\n",
      "Loss After Epoch 48  : 0.549486525774002\n",
      "Starting epoch 50\n",
      "Loss After Epoch 49  : 0.5459289889335632\n",
      "Starting epoch 51\n",
      "Loss After Epoch 50  : 0.5400461821556092\n",
      "Starting epoch 52\n",
      "Loss After Epoch 51  : 0.5372558281024297\n",
      "Starting epoch 53\n",
      "Loss After Epoch 52  : 0.5351163110733033\n",
      "Starting epoch 54\n",
      "Loss After Epoch 53  : 0.5314033753871917\n",
      "Starting epoch 55\n",
      "Loss After Epoch 54  : 0.5281630318164825\n",
      "Starting epoch 56\n",
      "Loss After Epoch 55  : 0.5245268109639486\n",
      "Starting epoch 57\n",
      "Loss After Epoch 56  : 0.5232948943376541\n",
      "Starting epoch 58\n",
      "Loss After Epoch 57  : 0.5195131789048513\n",
      "Starting epoch 59\n",
      "Loss After Epoch 58  : 0.5152808654705684\n",
      "Starting epoch 60\n",
      "Loss After Epoch 59  : 0.5124369776646296\n",
      "Starting epoch 61\n",
      "Loss After Epoch 60  : 0.5078201529184977\n",
      "Starting epoch 62\n",
      "Loss After Epoch 61  : 0.5053517282406489\n",
      "Starting epoch 63\n",
      "Loss After Epoch 62  : 0.5025154777765274\n",
      "Starting epoch 64\n",
      "Loss After Epoch 63  : 0.5013400160074234\n",
      "Starting epoch 65\n",
      "Loss After Epoch 64  : 0.49787126934528353\n",
      "Starting epoch 66\n",
      "Loss After Epoch 65  : 0.4922225036621094\n",
      "Starting epoch 67\n",
      "Loss After Epoch 66  : 0.4901848143140475\n",
      "Starting epoch 68\n",
      "Loss After Epoch 67  : 0.4888781286478043\n",
      "Starting epoch 69\n",
      "Loss After Epoch 68  : 0.48430648136138915\n",
      "Starting epoch 70\n",
      "Loss After Epoch 69  : 0.48233751207590103\n",
      "Starting epoch 71\n",
      "Loss After Epoch 70  : 0.47970627164840696\n",
      "Starting epoch 72\n",
      "Loss After Epoch 71  : 0.4753118315935135\n",
      "Starting epoch 73\n",
      "Loss After Epoch 72  : 0.47405931758880615\n",
      "Starting epoch 74\n",
      "Loss After Epoch 73  : 0.4713380862871806\n",
      "Starting epoch 75\n",
      "Loss After Epoch 74  : 0.46972177612781524\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "for epoch in range(0, 75):\n",
    "\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # Get inputs\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs.float())\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "    print(\"Loss After Epoch\",epoch+1,\" :\",current_loss/len(trainloader))\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e3145a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    \"\"\"\n",
    "        Function to get the predictions form a model\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        outputs = model(batch.cuda().float())\n",
    "        _, predicted = torch.max(outputs.data, 1) \n",
    "        predictions.append(predicted.cpu())\n",
    "    #predictions = np.array(predictions)\n",
    "    for i in range(len(predictions)):\n",
    "        predictions[i] = predictions[i].tolist()\n",
    "    predictions = list(chain.from_iterable(predictions))\n",
    "    predictions = [i+1 for i in predictions]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9050a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(x_te,batch_size=64, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a4d2c05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6430833333333333\n"
     ]
    }
   ],
   "source": [
    "# Evaluating MLP\n",
    "predictions = predict(mlp,test_loader)\n",
    "y = classification_report(predictions,y_test,output_dict=True)\n",
    "print(y[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8262178b",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ccd9fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_ten_concat_features(df):\n",
    "    \"\"\"\n",
    "    Function to get the first 10 words concatenated Embedding vector\n",
    "    \"\"\"\n",
    "    word_embeddings = []\n",
    "    for r in df[\"review_body\"]:\n",
    "        vec = np.empty( shape=(0,) )\n",
    "        c = 0\n",
    "        j = 0\n",
    "        while(c<10):\n",
    "            try:\n",
    "                a = wv[r[j]]\n",
    "                #print(\"BELLO!!\", c, r[j])\n",
    "                vec = np.concatenate([vec, a]) \n",
    "                c += 1\n",
    "                j += 1\n",
    "            except KeyError:\n",
    "                j += 1\n",
    "                continue\n",
    "                \n",
    "            except IndexError:\n",
    "                #print(\"BELLO!!\", c)\n",
    "                vec = np.concatenate([vec,np.zeros(300)])\n",
    "                c += 1\n",
    "    \n",
    "        word_embeddings.append(vec)\n",
    "    return word_embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "553dc94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(3000, 100).cuda(),\n",
    "      nn.LeakyReLU(),\n",
    "      nn.Linear(100, 10).cuda(),\n",
    "      nn.LeakyReLU(),\n",
    "      nn.Linear(10, 3).cuda(),\n",
    "      #nn.Dropout(0.2)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Forward pass'''\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "958c6965",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_first_ten_concat_features(X_train)\n",
    "trainloader = torch.utils.data.DataLoader(get_tuple(x,y_train),batch_size=64, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5acbc07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLP\n",
    "mlp = MLP2()\n",
    "mlp = mlp.cuda()\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba222c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss After Epoch 1  : 0.9310969703992208\n",
      "Starting epoch 2\n",
      "Loss After Epoch 2  : 0.8373928763071696\n",
      "Starting epoch 3\n",
      "Loss After Epoch 3  : 0.7719850514729818\n",
      "Starting epoch 4\n",
      "Loss After Epoch 4  : 0.6889080033302307\n",
      "Starting epoch 5\n",
      "Loss After Epoch 5  : 0.5877161382834116\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "for epoch in range(0, 5):\n",
    "\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # Get inputs\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs.float())\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "    print(\"Loss After Epoch\",epoch+1,\" :\",current_loss/len(trainloader))\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eae6c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(get_first_ten_concat_features(X_test),batch_size=64, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e09556c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5789166666666666\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the MLP\n",
    "predictions = predict(mlp,test_loader)\n",
    "y = classification_report(predictions,y_test,output_dict=True)\n",
    "print(y[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820b4de",
   "metadata": {},
   "source": [
    "**Q** What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section. <br>\n",
    "**Ans:** The Accuracy is very similar for the mean word embedding MLP. It is outperforming Perceptron by around 1.2% and underperforming to SVM by only 0.7%. On the other hand the second MLP which only takes the first 10 words is underperforming both Perceptron and SVM by 5.3% and 7.2% respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a626a8c9",
   "metadata": {},
   "source": [
    "## 5. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4eeb550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_twenty_concat_features(df):\n",
    "    \"\"\"\n",
    "    Function to get the First 20 words Embedding vector\n",
    "    \"\"\"\n",
    "    word_embeddings = []\n",
    "    for r in df[\"review_body\"]:\n",
    "        vec = []\n",
    "        c = 0\n",
    "        j = 0\n",
    "        while(c<20):\n",
    "            try:\n",
    "                a = wv[r[j]]\n",
    "                #print(\"BELLO!!\", c, r[j])\n",
    "                vec.append(a) \n",
    "                c += 1\n",
    "                j += 1\n",
    "            except KeyError:\n",
    "                j += 1\n",
    "                continue\n",
    "                \n",
    "            except IndexError:\n",
    "                #print(\"BELLO!!\", c)\n",
    "                vec.append(np.zeros(300))\n",
    "                c += 1\n",
    "    \n",
    "        word_embeddings.append(vec)\n",
    "    return word_embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89796d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs, trainloader, device, optimizer, loss_function):\n",
    "    \"\"\"\n",
    "        Function to train RNN based models.\n",
    "    \"\"\"\n",
    "    for epoch in range(0, epochs): # 5 epochs at maximum\n",
    "\n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "        model.train()\n",
    "        train_loss_seq = 0.0\n",
    "\n",
    "        for i, (train_data, target) in enumerate(trainloader):\n",
    "            train_data = train_data.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_data.float())\n",
    "            loss = loss_function(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_seq += loss.item()*train_data.size(0)\n",
    "        model.eval()\n",
    "\n",
    "        train_loss_seq = train_loss_seq/len(trainloader.dataset)\n",
    "        print(\"Epoch --> \"+str(epoch+1)+\" :\", train_loss_seq)\n",
    "    # Process is complete.\n",
    "    print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ba3d6",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b5163dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size=300, hidden_size=20,num_layers = 1,num_classes=3):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = torch.nn.RNN(input_size,hidden_size,num_layers,batch_first=True,dropout=0.5,nonlinearity='relu')\n",
    "        self.fc = torch.nn.Linear(hidden_size,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out,_ = self.rnn(x,h0)\n",
    "        out = out[:,-1,:]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "66d17a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vatsa\\anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# Initializing RNN Model\n",
    "rnn_model = RNN()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn_model.parameters(), lr=1e-3)\n",
    "rnn_model = rnn_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "872919d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,503 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(rnn_model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d893e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_first_twenty_concat_features(X_train)\n",
    "x = [np.asarray(i) for i in x]\n",
    "trainloader = torch.utils.data.DataLoader(get_tuple(x,y_train),batch_size=64, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3c07cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_te = get_first_twenty_concat_features(X_test)\n",
    "x_te = [np.asarray(i) for i in x_te]\n",
    "test_loader = torch.utils.data.DataLoader(x_te,batch_size=64, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f620ebb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch --> 1 : 1.0396501942475638\n",
      "Starting epoch 2\n",
      "Epoch --> 2 : 0.9036978100935618\n",
      "Starting epoch 3\n",
      "Epoch --> 3 : 0.8653938625653584\n",
      "Starting epoch 4\n",
      "Epoch --> 4 : 0.8430932471752167\n",
      "Starting epoch 5\n",
      "Epoch --> 5 : 0.8271982341607411\n",
      "Starting epoch 6\n",
      "Epoch --> 6 : 0.8164633108774821\n",
      "Starting epoch 7\n",
      "Epoch --> 7 : 0.8055796077251435\n",
      "Starting epoch 8\n",
      "Epoch --> 8 : 0.7996100840568543\n",
      "Starting epoch 9\n",
      "Epoch --> 9 : 0.789910701751709\n",
      "Starting epoch 10\n",
      "Epoch --> 10 : 0.7850388867060344\n",
      "Starting epoch 11\n",
      "Epoch --> 11 : 0.7788948087692261\n",
      "Starting epoch 12\n",
      "Epoch --> 12 : 0.7737591408888499\n",
      "Starting epoch 13\n",
      "Epoch --> 13 : 0.7696171850363414\n",
      "Starting epoch 14\n",
      "Epoch --> 14 : 0.7654237356980642\n",
      "Starting epoch 15\n",
      "Epoch --> 15 : 0.7645855046908061\n",
      "Starting epoch 16\n",
      "Epoch --> 16 : 0.7593455042044321\n",
      "Starting epoch 17\n",
      "Epoch --> 17 : 0.7556989444891612\n",
      "Starting epoch 18\n",
      "Epoch --> 18 : 0.7520121556917826\n",
      "Starting epoch 19\n",
      "Epoch --> 19 : 0.7495156970818837\n",
      "Starting epoch 20\n",
      "Epoch --> 20 : 0.7468056027094523\n",
      "Starting epoch 21\n",
      "Epoch --> 21 : 0.7419921885331472\n",
      "Starting epoch 22\n",
      "Epoch --> 22 : 0.7386032441457112\n",
      "Starting epoch 23\n",
      "Epoch --> 23 : 0.7399375410874685\n",
      "Starting epoch 24\n",
      "Epoch --> 24 : 0.7361737392743428\n",
      "Starting epoch 25\n",
      "Epoch --> 25 : 0.7331608528693517\n",
      "Starting epoch 26\n",
      "Epoch --> 26 : 0.730109917640686\n",
      "Starting epoch 27\n",
      "Epoch --> 27 : 0.7317570678393046\n",
      "Starting epoch 28\n",
      "Epoch --> 28 : 0.7286343031724294\n",
      "Starting epoch 29\n",
      "Epoch --> 29 : 0.7257773742278417\n",
      "Starting epoch 30\n",
      "Epoch --> 30 : 0.7267016449769338\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "train_model(rnn_model, 30, trainloader, device, optimizer, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "359994eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6465833333333333\n"
     ]
    }
   ],
   "source": [
    "# Evaluating RNN\n",
    "predictions = predict(rnn_model,test_loader)\n",
    "y = classification_report(predictions,y_test,output_dict=True)\n",
    "print(y[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab95f35",
   "metadata": {},
   "source": [
    "**Q:** What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models. <br>\n",
    "**Ans:** The RNN is outperforming both the Feed Forward neural networks. The first one by 0.3% and the second one by a large 6.8% difference in the accuracy score. As it is considering the dependency of previous words in the review it is able to differentiate better than the feedforward neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f7d592",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad296b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(torch.nn.Module):\n",
    " \n",
    "    def __init__(self, input_size=300, hidden_size=20,num_layers = 1,num_classes=3):\n",
    "        super(GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = torch.nn.GRU(input_size,hidden_size,num_layers,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size,num_classes,bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out,_ = self.gru(x,h0)\n",
    "        out = out[:,-1,:]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "99dfc49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing GRU\n",
    "gru_model = GRU()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gru_model.parameters(), lr=1e-3)\n",
    "gru_model = gru_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "69520563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 19,380 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'The model has {count_parameters(gru_model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8efc0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch --> 1 : 0.9434848532676697\n",
      "Starting epoch 2\n",
      "Epoch --> 2 : 0.8038084135055542\n",
      "Starting epoch 3\n",
      "Epoch --> 3 : 0.7748165690898895\n",
      "Starting epoch 4\n",
      "Epoch --> 4 : 0.7536724455356598\n",
      "Starting epoch 5\n",
      "Epoch --> 5 : 0.7379129912853241\n",
      "Starting epoch 6\n",
      "Epoch --> 6 : 0.7261548719803492\n",
      "Starting epoch 7\n",
      "Epoch --> 7 : 0.7139907979170481\n",
      "Starting epoch 8\n",
      "Epoch --> 8 : 0.7038073006073634\n",
      "Starting epoch 9\n",
      "Epoch --> 9 : 0.6941189201672872\n",
      "Starting epoch 10\n",
      "Epoch --> 10 : 0.6851310456593831\n",
      "Starting epoch 11\n",
      "Epoch --> 11 : 0.6782764012813568\n",
      "Starting epoch 12\n",
      "Epoch --> 12 : 0.6699842751026154\n",
      "Starting epoch 13\n",
      "Epoch --> 13 : 0.6639130733410518\n",
      "Starting epoch 14\n",
      "Epoch --> 14 : 0.6566061506271362\n",
      "Starting epoch 15\n",
      "Epoch --> 15 : 0.6497284882863362\n",
      "Starting epoch 16\n",
      "Epoch --> 16 : 0.6450611269474029\n",
      "Starting epoch 17\n",
      "Epoch --> 17 : 0.6398731488784154\n",
      "Starting epoch 18\n",
      "Epoch --> 18 : 0.6330426509777705\n",
      "Starting epoch 19\n",
      "Epoch --> 19 : 0.6271306569973628\n",
      "Starting epoch 20\n",
      "Epoch --> 20 : 0.6230615017016728\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "train_model(gru_model, 20, trainloader, device, optimizer, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fca1d5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6685\n"
     ]
    }
   ],
   "source": [
    "#Evaluating GRU\n",
    "predictions = predict(gru_model,test_loader)\n",
    "y = classification_report(predictions,y_test,output_dict=True)\n",
    "print(y[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b044da",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "55ee088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size=300, hidden_size=20,num_layers = 1,num_classes=3):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = torch.nn.LSTM(input_size,hidden_size,num_layers,batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size,num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out,_ = self.lstm(x,(h0,c0))\n",
    "        out = out[:,-1,:]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9fad320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing LSTM\n",
    "lstm_model = LSTM()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=1e-3)\n",
    "lstm_model = lstm_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e19559de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 25,823 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'The model has {count_parameters(lstm_model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e7ea0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch --> 1 : 0.9605202089945475\n",
      "Starting epoch 2\n",
      "Epoch --> 2 : 0.8394982452392579\n",
      "Starting epoch 3\n",
      "Epoch --> 3 : 0.7956503780682882\n",
      "Starting epoch 4\n",
      "Epoch --> 4 : 0.7715079264640808\n",
      "Starting epoch 5\n",
      "Epoch --> 5 : 0.7535925794045131\n",
      "Starting epoch 6\n",
      "Epoch --> 6 : 0.74114122402668\n",
      "Starting epoch 7\n",
      "Epoch --> 7 : 0.7280267721017202\n",
      "Starting epoch 8\n",
      "Epoch --> 8 : 0.7159666108687719\n",
      "Starting epoch 9\n",
      "Epoch --> 9 : 0.7065933414697647\n",
      "Starting epoch 10\n",
      "Epoch --> 10 : 0.6971026578744253\n",
      "Starting epoch 11\n",
      "Epoch --> 11 : 0.6890288486083349\n",
      "Starting epoch 12\n",
      "Epoch --> 12 : 0.6797167123953501\n",
      "Starting epoch 13\n",
      "Epoch --> 13 : 0.6716538064082463\n",
      "Starting epoch 14\n",
      "Epoch --> 14 : 0.6662318698565165\n",
      "Starting epoch 15\n",
      "Epoch --> 15 : 0.658796049674352\n",
      "Starting epoch 16\n",
      "Epoch --> 16 : 0.6529413114786148\n",
      "Starting epoch 17\n",
      "Epoch --> 17 : 0.645723607579867\n",
      "Starting epoch 18\n",
      "Epoch --> 18 : 0.6404907112121582\n",
      "Starting epoch 19\n",
      "Epoch --> 19 : 0.6333893365462621\n",
      "Starting epoch 20\n",
      "Epoch --> 20 : 0.6285951400200526\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "train_model(lstm_model, 20, trainloader, device, optimizer, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b4de5eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6601666666666667\n"
     ]
    }
   ],
   "source": [
    "#Evaluating LSTM\n",
    "predictions = predict(lstm_model,test_loader)\n",
    "y = classification_report(predictions,y_test,output_dict=True)\n",
    "print(y[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4990cbe6",
   "metadata": {},
   "source": [
    "**Q** What do you conclude by comparing accuracy values you obtain by GRU, LSTM, and simple RNN. <br>\n",
    "**Ans:** LSTM and GRU both outperforms RNN by 1.4% and 2.2% respectively. These can be because of the more number of parameters that LSTM and GRU has. Both LSTM and GRU are giving similar performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616c2d41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
